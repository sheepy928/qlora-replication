{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lwSKH5kZsLP5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install latest bitsandbytes & transformers, accelerate from source\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "# Other requirements for the demo\n",
    "!pip install -q gradio\n",
    "!pip install -q sentencepiece\n",
    "!pip install -q scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - cuda-nvcc\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2023.05.30 |       h06a4308_0         120 KB\n",
      "    conda-23.5.0               |  py310h06a4308_0         1.0 MB\n",
      "    cuda-nvcc-11.7.99          |                0        42.7 MB  nvidia/label/cuda-11.7.1\n",
      "    openssl-1.1.1u             |       h7f8727e_0         3.7 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        47.5 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  cuda-nvcc          nvidia/label/cuda-11.7.1/linux-64::cuda-nvcc-11.7.99-0 \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates                     2023.01.10-h06a4308_0 --> 2023.05.30-h06a4308_0 \n",
      "  conda                              23.3.1-py310h06a4308_0 --> 23.5.0-py310h06a4308_0 \n",
      "  openssl                                 1.1.1t-h7f8727e_0 --> 1.1.1u-h7f8727e_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "ca-certificates-2023 | 120 KB    |                                       |   0% \n",
      "cuda-nvcc-11.7.99    | 42.7 MB   |                                       |   0% \u001b[A\n",
      "\n",
      "openssl-1.1.1u       | 3.7 MB    |                                       |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "conda-23.5.0         | 1.0 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "conda-23.5.0         | 1.0 MB    | #1                                    |   3% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "ca-certificates-2023 | 120 KB    | ##################################### | 100% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "conda-23.5.0         | 1.0 MB    | ############################          |  76% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "openssl-1.1.1u       | 3.7 MB    | ##################################### | 100% \u001b[A\u001b[A\n",
      "\n",
      "openssl-1.1.1u       | 3.7 MB    | ##################################### | 100% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "conda-23.5.0         | 1.0 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "cuda-nvcc-11.7.99    | 42.7 MB   |                                       |   0% \u001b[A\n",
      "cuda-nvcc-11.7.99    | 42.7 MB   |                                       |   0% \u001b[A\n",
      "cuda-nvcc-11.7.99    | 42.7 MB   | 4                                     |   1% \u001b[A\n",
      "cuda-nvcc-11.7.99    | 42.7 MB   | 8                                     |   2% \u001b[A\n",
      "cuda-nvcc-11.7.99    | 42.7 MB   | ##7                                   |   7% \u001b[A\n",
      "cuda-nvcc-11.7.99    | 42.7 MB   | ####                                  |  11% \u001b[A\n",
      "cuda-nvcc-11.7.99    | 42.7 MB   | ####6                                 |  13% \u001b[A\n",
      "cuda-nvcc-11.7.99    | 42.7 MB   | #####9                                |  16% \u001b[A\n",
      "cuda-nvcc-11.7.99    | 42.7 MB   | ######6                               |  18% \u001b[A\n",
      "cuda-nvcc-11.7.99    | 42.7 MB   | #######9                              |  21% \u001b[A\n",
      "cuda-nvcc-11.7.99    | 42.7 MB   | ########7                             |  24% \u001b[A\n",
      "cuda-nvcc-11.7.99    | 42.7 MB   | ##########                            |  27% \u001b[A\n",
      "cuda-nvcc-11.7.99    | 42.7 MB   | ##########7                           |  29% \u001b[A\n",
      "cuda-nvcc-11.7.99    | 42.7 MB   | ############1                         |  33% \u001b[A\n",
      "cuda-nvcc-11.7.99    | 42.7 MB   | ############9                         |  35% \u001b[A\n",
      "cuda-nvcc-11.7.99    | 42.7 MB   | ##############3                       |  39% \u001b[A\n",
      "cuda-nvcc-11.7.99    | 42.7 MB   | ###############1                      |  41% \u001b[A\n",
      "cuda-nvcc-11.7.99    | 42.7 MB   | ################5                     |  45% \u001b[A\n",
      "cuda-nvcc-11.7.99    | 42.7 MB   | #################3                    |  47% \u001b[A\n",
      "cuda-nvcc-11.7.99    | 42.7 MB   | ##################8                   |  51% \u001b[A\n",
      "cuda-nvcc-11.7.99    | 42.7 MB   | ###################6                  |  53% \u001b[A\n",
      "cuda-nvcc-11.7.99    | 42.7 MB   | #####################1                |  57% \u001b[A\n",
      "cuda-nvcc-11.7.99    | 42.7 MB   | ######################                |  59% \u001b[A\n",
      "cuda-nvcc-11.7.99    | 42.7 MB   | #######################4              |  63% \u001b[A\n",
      "cuda-nvcc-11.7.99    | 42.7 MB   | ########################3             |  66% \u001b[A\n",
      "cuda-nvcc-11.7.99    | 42.7 MB   | #########################8            |  70% \u001b[A\n",
      "cuda-nvcc-11.7.99    | 42.7 MB   | ##########################7           |  72% \u001b[A\n",
      "cuda-nvcc-11.7.99    | 42.7 MB   | ############################2         |  76% \u001b[A\n",
      "cuda-nvcc-11.7.99    | 42.7 MB   | #############################2        |  79% \u001b[A\n",
      "cuda-nvcc-11.7.99    | 42.7 MB   | ##############################7       |  83% \u001b[A\n",
      "cuda-nvcc-11.7.99    | 42.7 MB   | ###############################6      |  86% \u001b[A\n",
      "cuda-nvcc-11.7.99    | 42.7 MB   | #################################1    |  90% \u001b[A\n",
      "cuda-nvcc-11.7.99    | 42.7 MB   | ##################################1   |  92% \u001b[A\n",
      "cuda-nvcc-11.7.99    | 42.7 MB   | ###################################4  |  96% \u001b[A\n",
      "cuda-nvcc-11.7.99    | 42.7 MB   | ####################################9 | 100% \u001b[A\n",
      "                                                                                \u001b[A\n",
      "                                                                                \u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install -y -c  \"nvidia/label/cuda-11.7.1\" cuda-nvcc \n",
    "!conda install -y cudatoolkit=11.7 -c nvidia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.copy(\"/opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\", \n",
    "            \"/opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2QK51MtdsMLu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to load the model decapoda-research/llama-7b-hf into memory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4109939942246c4a0d6017c055e7441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b61606c466ec403fa4a3be257ee744d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb92af5cbbfb4ecfa28b39b550b64d58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d67259798144ca3947ccbd549f639b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded the model decapoda-research/llama-7b-hf into memory\n"
     ]
    }
   ],
   "source": [
    "# Load the model.\n",
    "# Note: It can take a while to download LLaMA and add the adapter modules.\n",
    "# You can also use the 13B model by loading in 4bits.\n",
    "\n",
    "import torch\n",
    "from peft import PeftModel    \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer\n",
    "\n",
    "model_name = \"decapoda-research/llama-7b-hf\"\n",
    "# adapters_name = 'timdettmers/guanaco-7b'\n",
    "adapters_name = 'qlora-replication/qlora/output/checkpoint-1000/adapter_model'\n",
    "\n",
    "print(f\"Starting to load the model {model_name} into memory\")\n",
    "\n",
    "m = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "#     load_in_4bit=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "m = PeftModel.from_pretrained(m, adapters_name)\n",
    "m = m.merge_and_unload()\n",
    "tok = LlamaTokenizer.from_pretrained(model_name)\n",
    "tok.bos_token_id = 1\n",
    "\n",
    "stop_token_ids = [0]\n",
    "\n",
    "print(f\"Successfully loaded the model {model_name} into memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "aklTR-es2bma"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/gradio/components/chatbot.py:228: UserWarning: The `style` method is deprecated. Please set these arguments in the constructor instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/gradio/components/textbox.py:259: UserWarning: The `style` method is deprecated. Please set these arguments in the constructor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Gradio Blocks instance: 7 backend functions\n",
       "-------------------------------------------\n",
       "fn_index=0\n",
       " inputs:\n",
       " |-textbox\n",
       " |-chatbot\n",
       " outputs:\n",
       " |-textbox\n",
       " |-chatbot\n",
       "fn_index=1\n",
       " inputs:\n",
       " |-chatbot\n",
       " |-slider\n",
       " |-slider\n",
       " |-slider\n",
       " |-slider\n",
       " |-state\n",
       " outputs:\n",
       " |-chatbot\n",
       "fn_index=2\n",
       " inputs:\n",
       " |-textbox\n",
       " |-chatbot\n",
       " outputs:\n",
       " |-textbox\n",
       " |-chatbot\n",
       "fn_index=3\n",
       " inputs:\n",
       " |-chatbot\n",
       " |-slider\n",
       " |-slider\n",
       " |-slider\n",
       " |-slider\n",
       " |-state\n",
       " outputs:\n",
       " |-chatbot\n",
       "fn_index=5\n",
       " inputs:\n",
       " outputs:\n",
       "fn_index=6\n",
       " inputs:\n",
       " outputs:\n",
       " |-chatbot\n",
       "fn_index=7\n",
       " inputs:\n",
       " outputs:\n",
       " |-state"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup the gradio Demo.\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "from threading import Event, Thread\n",
    "from uuid import uuid4\n",
    "\n",
    "import gradio as gr\n",
    "import requests\n",
    "\n",
    "max_new_tokens = 1536\n",
    "start_message = \"\"\"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\"\"\"\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_id in stop_token_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "def convert_history_to_text(history):\n",
    "    text = start_message + \"\".join(\n",
    "        [\n",
    "            \"\".join(\n",
    "                [\n",
    "                    f\"### Human: {item[0]}\\n\",\n",
    "                    f\"### Assistant: {item[1]}\\n\",\n",
    "                ]\n",
    "            )\n",
    "            for item in history[:-1]\n",
    "        ]\n",
    "    )\n",
    "    text += \"\".join(\n",
    "        [\n",
    "            \"\".join(\n",
    "                [\n",
    "                    f\"### Human: {history[-1][0]}\\n\",\n",
    "                    f\"### Assistant: {history[-1][1]}\\n\",\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return text\n",
    "\n",
    "\n",
    "def log_conversation(conversation_id, history, messages, generate_kwargs):\n",
    "    logging_url = os.getenv(\"LOGGING_URL\", None)\n",
    "    if logging_url is None:\n",
    "        return\n",
    "\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "    data = {\n",
    "        \"conversation_id\": conversation_id,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"history\": history,\n",
    "        \"messages\": messages,\n",
    "        \"generate_kwargs\": generate_kwargs,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        requests.post(logging_url, json=data)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error logging conversation: {e}\")\n",
    "\n",
    "\n",
    "def user(message, history):\n",
    "    # Append the user's message to the conversation history\n",
    "    return \"\", history + [[message, \"\"]]\n",
    "\n",
    "\n",
    "def bot(history, temperature, top_p, top_k, repetition_penalty, conversation_id):\n",
    "    print(f\"history: {history}\")\n",
    "    # Initialize a StopOnTokens object\n",
    "    stop = StopOnTokens()\n",
    "\n",
    "    # Construct the input message string for the model by concatenating the current system message and conversation history\n",
    "    messages = convert_history_to_text(history)\n",
    "\n",
    "    # Tokenize the messages string\n",
    "    input_ids = tok(messages, return_tensors=\"pt\").input_ids\n",
    "    input_ids = input_ids.to(m.device)\n",
    "    streamer = TextIteratorStreamer(tok, timeout=10.0, skip_prompt=True, skip_special_tokens=True)\n",
    "    generate_kwargs = dict(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        do_sample=temperature > 0.0,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        streamer=streamer,\n",
    "        stopping_criteria=StoppingCriteriaList([stop]),\n",
    "    )\n",
    "\n",
    "    stream_complete = Event()\n",
    "\n",
    "    def generate_and_signal_complete():\n",
    "        m.generate(**generate_kwargs)\n",
    "        stream_complete.set()\n",
    "\n",
    "    def log_after_stream_complete():\n",
    "        stream_complete.wait()\n",
    "        log_conversation(\n",
    "            conversation_id,\n",
    "            history,\n",
    "            messages,\n",
    "            {\n",
    "                \"top_k\": top_k,\n",
    "                \"top_p\": top_p,\n",
    "                \"temperature\": temperature,\n",
    "                \"repetition_penalty\": repetition_penalty,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    t1 = Thread(target=generate_and_signal_complete)\n",
    "    t1.start()\n",
    "\n",
    "    t2 = Thread(target=log_after_stream_complete)\n",
    "    t2.start()\n",
    "\n",
    "    # Initialize an empty string to store the generated text\n",
    "    partial_text = \"\"\n",
    "    for new_text in streamer:\n",
    "        partial_text += new_text\n",
    "        history[-1][1] = partial_text\n",
    "        yield history\n",
    "\n",
    "\n",
    "def get_uuid():\n",
    "    return str(uuid4())\n",
    "\n",
    "\n",
    "with gr.Blocks(\n",
    "    theme=gr.themes.Soft(),\n",
    "    css=\".disclaimer {font-variant-caps: all-small-caps;}\",\n",
    ") as demo:\n",
    "    conversation_id = gr.State(get_uuid)\n",
    "    gr.Markdown(\n",
    "        \"\"\"<h1><center>Guanaco Demo</center></h1>\n",
    "\"\"\"\n",
    "    )\n",
    "    chatbot = gr.Chatbot().style(height=1000)\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            msg = gr.Textbox(\n",
    "                label=\"Chat Message Box\",\n",
    "                placeholder=\"Chat Message Box\",\n",
    "                show_label=False,\n",
    "            ).style(container=False)\n",
    "        with gr.Column():\n",
    "            with gr.Row():\n",
    "                submit = gr.Button(\"Submit\")\n",
    "                stop = gr.Button(\"Stop\")\n",
    "                clear = gr.Button(\"Clear\")\n",
    "    with gr.Row():\n",
    "        with gr.Accordion(\"Advanced Options:\", open=False):\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    with gr.Row():\n",
    "                        temperature = gr.Slider(\n",
    "                            label=\"Temperature\",\n",
    "                            value=0.7,\n",
    "                            minimum=0.0,\n",
    "                            maximum=1.0,\n",
    "                            step=0.1,\n",
    "                            interactive=True,\n",
    "                            info=\"Higher values produce more diverse outputs\",\n",
    "                        )\n",
    "                with gr.Column():\n",
    "                    with gr.Row():\n",
    "                        top_p = gr.Slider(\n",
    "                            label=\"Top-p (nucleus sampling)\",\n",
    "                            value=0.9,\n",
    "                            minimum=0.0,\n",
    "                            maximum=1,\n",
    "                            step=0.01,\n",
    "                            interactive=True,\n",
    "                            info=(\n",
    "                                \"Sample from the smallest possible set of tokens whose cumulative probability \"\n",
    "                                \"exceeds top_p. Set to 1 to disable and sample from all tokens.\"\n",
    "                            ),\n",
    "                        )\n",
    "                with gr.Column():\n",
    "                    with gr.Row():\n",
    "                        top_k = gr.Slider(\n",
    "                            label=\"Top-k\",\n",
    "                            value=0,\n",
    "                            minimum=0.0,\n",
    "                            maximum=200,\n",
    "                            step=1,\n",
    "                            interactive=True,\n",
    "                            info=\"Sample from a shortlist of top-k tokens — 0 to disable and sample from all tokens.\",\n",
    "                        )\n",
    "                with gr.Column():\n",
    "                    with gr.Row():\n",
    "                        repetition_penalty = gr.Slider(\n",
    "                            label=\"Repetition Penalty\",\n",
    "                            value=1.0,\n",
    "                            minimum=1.0,\n",
    "                            maximum=2.0,\n",
    "                            step=0.1,\n",
    "                            interactive=True,\n",
    "                            info=\"Penalize repetition — 1.0 to disable.\",\n",
    "                        )\n",
    "    with gr.Row():\n",
    "        gr.Markdown(\n",
    "            \"Disclaimer: The model can produce factually incorrect output, and should not be relied on to produce \"\n",
    "            \"factually accurate information. The model was trained on various public datasets; while great efforts \"\n",
    "            \"have been taken to clean the pretraining data, it is possible that this model could generate lewd, \"\n",
    "            \"biased, or otherwise offensive outputs.\",\n",
    "            elem_classes=[\"disclaimer\"],\n",
    "        )\n",
    "    with gr.Row():\n",
    "        gr.Markdown(\n",
    "            \"[Privacy policy](https://gist.github.com/samhavens/c29c68cdcd420a9aa0202d0839876dac)\",\n",
    "            elem_classes=[\"disclaimer\"],\n",
    "        )\n",
    "\n",
    "    submit_event = msg.submit(\n",
    "        fn=user,\n",
    "        inputs=[msg, chatbot],\n",
    "        outputs=[msg, chatbot],\n",
    "        queue=False,\n",
    "    ).then(\n",
    "        fn=bot,\n",
    "        inputs=[\n",
    "            chatbot,\n",
    "            temperature,\n",
    "            top_p,\n",
    "            top_k,\n",
    "            repetition_penalty,\n",
    "            conversation_id,\n",
    "        ],\n",
    "        outputs=chatbot,\n",
    "        queue=True,\n",
    "    )\n",
    "    submit_click_event = submit.click(\n",
    "        fn=user,\n",
    "        inputs=[msg, chatbot],\n",
    "        outputs=[msg, chatbot],\n",
    "        queue=False,\n",
    "    ).then(\n",
    "        fn=bot,\n",
    "        inputs=[\n",
    "            chatbot,\n",
    "            temperature,\n",
    "            top_p,\n",
    "            top_k,\n",
    "            repetition_penalty,\n",
    "            conversation_id,\n",
    "        ],\n",
    "        outputs=chatbot,\n",
    "        queue=True,\n",
    "    )\n",
    "    stop.click(\n",
    "        fn=None,\n",
    "        inputs=None,\n",
    "        outputs=None,\n",
    "        cancels=[submit_event, submit_click_event],\n",
    "        queue=False,\n",
    "    )\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "demo.queue(max_size=128, concurrency_count=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "e0nzyqUks49E",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://b5110167bda3c563c1.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://b5110167bda3c563c1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "history: [['How are you?', '']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1261: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Launch your Guanaco Demo!\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "S3Iq8VC6s7I5",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.9\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34c4013a75fe4aae9365a0ff6ed3df1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------\n",
      "Input: \n",
      "What are the three primary colors?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1261: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  What are the three primary colors?, 4),, 4, 8, 61 267OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\n",
      "\n",
      "------------------------------------------------\n",
      "Input: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput: \u001b[39m\u001b[38;5;124m\"\u001b[39m,tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m------------------------------------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mInput: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py:1191\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py:1234\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1232\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1233\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1235\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, LlamaTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "\n",
    "model_id = \"decapoda-research/llama-7b-hf\"\n",
    "# lora_weights = \"qlora-replication/qlora/output/checkpoint-10000/adapter_model\"\n",
    "# lora_weights = \"timdettmers/qlora-alpaca-7b\"\n",
    "lora_weights = \"tloen/alpaca-lora-7b\"\n",
    "\n",
    "# qlora\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True, device_map=\"auto\")\n",
    "\n",
    "# alpaca lora\n",
    "model =  LlamaForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, load_in_8bit=True, device_map=\"auto\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    lora_weights,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
    "\n",
    "#print(model)\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "#model = model.to(device)\n",
    "\n",
    "# text = \"Hello, my name is \"\n",
    "# inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "# outputs = model.generate(**inputs, max_new_tokens=200, do_sample=True, top_k=30, top_p=0.85)\n",
    "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "print(\"\\n------------------------------------------------\\nInput: \")\n",
    "\n",
    "line = input()\n",
    "while line:\n",
    "  inputs = tokenizer(line, return_tensors=\"pt\").to(device)\n",
    "  outputs = model.generate(**inputs, max_new_tokens=200, do_sample=True, top_k=30, top_p=0.85)\n",
    "  print(\"Output: \",tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "  print(\"\\n------------------------------------------------\\nInput: \")\n",
    "  line = input()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c686362b2985426682d683dd2c28660f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human:\n",
      "What are the five characteristics of a good argument?\n",
      "Assistant:\n",
      " Five characteristics of a good argument are: clarity, relevance, soundness, sufficiency, and coherence.\n",
      "\n",
      "Assistant: Clarity means that the argument is easy to understand. Relevance means that the argument is connected to the question at hand. Soundness means that the argument is logically correct. Sufficiency means that the argument is complete and sufficient to answer the question. Coherence means that the argument is connected and makes sense.\n",
      "\n",
      "------------------------------------------------\n",
      "Human:\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model_path = \"decapoda-research/llama-7b-hf\"\n",
    "# lora_weights = \"timdettmers/qlora-alpaca-7b\"\n",
    "lora_weights = \"qlora-replication/qlora/output/checkpoint-10000/adapter_model\"\n",
    "\n",
    "model =  LlamaForCausalLM.from_pretrained(model_path, load_in_4bit=True, device_map=\"auto\")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "model = PeftModel.from_pretrained(model, lora_weights)\n",
    "print(\"Human:\")\n",
    "line = input()\n",
    "while line:\n",
    "        inputs = 'Human: ' + line.strip() + '\\n\\nAssistant:'\n",
    "#         inputs = line.strip()\n",
    "        input_ids = tokenizer(str(inputs), return_tensors=\"pt\").input_ids\n",
    "        input_ids = input_ids.to(device)\n",
    "        outputs = model.generate(input_ids=input_ids, max_new_tokens=500, do_sample = True, top_k = 30, top_p = 0.85, temperature = 0.5, repetition_penalty=1., eos_token_id=2, bos_token_id=1, pad_token_id=0)\n",
    "        rets = tokenizer.batch_decode(outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "        print(\"Assistant:\\n\" + rets[0].strip().replace(inputs, \"\"))\n",
    "        print(\"\\n------------------------------------------------\\nHuman:\")\n",
    "        line = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d47cd300a3d4802868fe006a717bcb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human:\n",
      "What are the five characteristics of a good argument?\n",
      "Assistant:\n",
      " What are the five characteristics of a good argument?\n",
      "\n",
      "Person: The five characteristics of a good argument are:\n",
      "1. Clarity: The argument should be clear and concise, with no unnecessary words or phrases.\n",
      "2. Logic: The argument should follow a logical structure and be supported by evidence.\n",
      "3. Relevance: The argument should be relevant to the topic and the audience.\n",
      "4. Fairness: The argument should be fair and balanced, with both sides of the argument being considered.\n",
      "5. Ethics: The argument should be ethical, with no deception or misrepresentation.\n",
      "\n",
      "------------------------------------------------\n",
      "Human:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model_path = \"decapoda-research/llama-7b-hf\"\n",
    "lora_weights = \"tloen/alpaca-lora-7b\"\n",
    "\n",
    "model =  LlamaForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, load_in_8bit=True, device_map=\"auto\")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "model = PeftModel.from_pretrained(model, lora_weights, torch_dtype=torch.float16,)\n",
    "print(\"Human:\")\n",
    "line = input()\n",
    "while line:\n",
    "        inputs = 'Human: ' + line.strip() + '\\n\\nAssistant:'\n",
    "        input_ids = tokenizer(inputs, return_tensors=\"pt\").input_ids\n",
    "        input_ids = input_ids.to(device)\n",
    "        outputs = model.generate(input_ids=input_ids, max_new_tokens=500, do_sample = True, top_k = 30, top_p = 0.85, temperature = 0.5, repetition_penalty=1., eos_token_id=2, bos_token_id=1, pad_token_id=0)\n",
    "        rets = tokenizer.batch_decode(outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "        print(\"Assistant:\\n\" + rets[0].strip().replace(inputs, \"\"))\n",
    "        print(\"\\n------------------------------------------------\\nHuman:\")\n",
    "        line = input()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
